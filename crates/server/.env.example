# -----------------------------------------------------------------------------
# Required Environment Variables
# -----------------------------------------------------------------------------
# Copy this file to .env and fill in your actual values.

# Your API key for the selected AI Provider (e.g., Google Gemini).
# This is a secret and should not be committed to version control.
AI_API_KEY=your_api_key_here

# The ID of your Google Cloud project where BigQuery is enabled.
BIGQUERY_PROJECT_ID=your-gcp-project-id

# The full URL for the AI provider's API endpoint.
# IMPORTANT: Do NOT wrap the URL in quotes.
AI_API_URL=https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent

# -----------------------------------------------------------------------------
# Optional Environment Variables
# -----------------------------------------------------------------------------
# These variables have sensible defaults but can be uncommented and changed.

# The AI provider to use. Can be "gemini" or "local". Defaults to "gemini".
# AI_PROVIDER=gemini

# For the "local" provider, you can specify a model name.
# AI_MODEL=your_local_model_name

# The port for the server to listen on inside the container. Defaults to 8080.
# PORT=8080

# Sets the logging level. Valid levels: trace, debug, info, warn, error
# RUST_LOG=info

# -----------------------------------------------------------------------------
# Prompt Customization (Optional)
# -----------------------------------------------------------------------------

# Override the default system prompt for query generation.
# This is useful for changing the AI's core behavior or persona.
# SYSTEM_PROMPT_TEMPLATE="You are a helpful pirate who translates English to pirate-speak."

# Override the default user prompt template for query generation.
# Available placeholders: {language}, {context}, {prompt}, {alias_instruction}
# USER_PROMPT_TEMPLATE="Given this context: {context}. Now, answer this question: {prompt}"
